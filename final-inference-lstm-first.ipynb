{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is the inference kernel for BERT+LSTM ensemble. Runs in Kaggle kernel in <2 hrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "package_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\n",
    "sys.path.append(package_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import os, glob, gc\n",
    "import warnings\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam, BertConfig\n",
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertModel\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from unicodedata import category, name, normalize\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "from string import ascii_uppercase\n",
    "import unicodedata as ud\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 256\n",
    "BERT_LARGE_BATCH_SIZE = 64\n",
    "BERT_BASE_BATCH_SIZE = 128\n",
    "LARGE_BIN_PATH = '../input/large-bin-all/'\n",
    "BASE_BIN_PATH = '../input/base-bin-all/'\n",
    "LARGE_CONT_PATH = '../input/large-cont-all/'\n",
    "BASE_CONT_PATH = '../input/base-cont-all/'\n",
    "MODELS_TO_SKIP = ['../input/large-cont-all/1_bert_pytorch_14_NewLoss_all_0.bin',\n",
    "                  '../input/large-cont-all/1_bert_pytorch_14_NewLoss_all_1.bin',\n",
    "                  '../input/large-bin-all/1_bert_pytorch_15_binary_all_0.bin',\n",
    "                  '../input/large-bin-all/1_bert_pytorch_15_binary_all_1.bin']\n",
    "\n",
    "# !!!!!  REPLACE (if not full test)!!!\n",
    "test_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n",
    "test_df['comment_text'] = test_df['comment_text'].astype(str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing for bert with binary target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lines_bin(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    lens = []\n",
    "    for text in example:\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        lens.append(len(tokens_a))\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    return np.array(all_tokens), lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing for bert with continoues target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolate_words = ['trump', 'clinton', 'penis', 'fuck', 'hillary']\n",
    "def isolate(text):\n",
    "    for word in isolate_words:\n",
    "        if word in text:\n",
    "            text = text.replace(word, ' '+word+' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "mis_spell_mapping = {'hwhat':'what',\n",
    "                      'whwhat': 'what', 'howhat':'how that',\n",
    "                      'whybis':'Why is', \n",
    "                      \"howddo\":\"How do\", 'howeber':'However', 'showh':'show',\n",
    "                      'fcuk':'fuck', 'fuuck':'fuck','fuuuck':'fuck'}\n",
    "\n",
    "def correct_misspel(text):\n",
    "    for error in mis_spell_mapping:\n",
    "        if error in text:\n",
    "            text = text.replace(error, mis_spell_mapping[error])        \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_diacritics(s):\n",
    "    return ''.join(c for c in normalize('NFKD', s.replace('ø', 'o').replace('Ø', 'O').replace('⁻', '-').replace('₋', '-'))\n",
    "                  if category(c) != 'Mn')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "letters = (x for x in ascii_uppercase if x not in ('Q', 'X'))\n",
    "mapping = {ord(ud.lookup('LATIN LETTER SMALL CAPITAL ' + x)): x for x in letters}\n",
    "\n",
    "\n",
    "def convert_line_cont(text, max_seq_length, tokenizer):\n",
    "    text = ' '.join(re.findall(r'[A-Z]?[^A-Z\\s]+|[A-Z]+', text))\n",
    "    text = re.sub(r'[A-Za-z]+://[A-Za-z0-9-_]+.[A-Za-z0-9-_:%&;\\?#/.=]+', ' https ', text)\n",
    "    text = text.lower()\n",
    "    text = correct_misspel(isolate(remove_diacritics(text)))\n",
    "    tokens_a = tokenizer.tokenize(text)\n",
    "    if len(tokens_a) > max_seq_length:\n",
    "        tokens_a = tokens_a[:max_seq_length]\n",
    "    return tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens_a + [\"[SEP]\"]) + [0] * (\n",
    "                max_seq_length - len(tokens_a)), len(tokens_a)\n",
    "\n",
    "\n",
    "def convert_lines_cont(examples, max_seq_length, tokenizer):\n",
    "    all_tokens = []\n",
    "    max_seq_length -= 2\n",
    "    examples = examples.str.translate(mapping)\n",
    "            \n",
    "    with Pool(2) as p:\n",
    "        all_tokens = p.starmap(convert_line_cont, [(text, max_seq_length, tokenizer) for text in examples])\n",
    "    \n",
    "    all_tokens, lens = map(list, zip(*all_tokens))\n",
    "    return np.array(all_tokens), lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tensors(tsrs):\n",
    "    max_len = max(torch.sum((torch.sum((tsrs != 0), dim=0) > 0)).item(), 1)\n",
    "    return tsrs[:, :max_len]\n",
    "\n",
    "\n",
    "def bert_model_inference(bert_config, model_path, test_loader, batch_size):\n",
    "    model = BertForSequenceClassification(bert_config, num_labels=7)\n",
    "    model.to(device)\n",
    "    \n",
    "    tmp_state = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(tmp_state)\n",
    "    del tmp_state\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model = model.eval()\n",
    "    \n",
    "    test_preds = np.zeros((len(X_test)))\n",
    "    \n",
    "    tk0 = tqdm(test_loader)\n",
    "    for i, (x_batch,) in enumerate(tk0):\n",
    "        x_batch = trim_tensors(x_batch)\n",
    "        pred = model(x_batch.to(device), \n",
    "                     attention_mask=(x_batch > 0).to(device))\n",
    "        test_preds[i * batch_size:(i + 1) * batch_size] = pred[:, 0].detach().cpu().squeeze().numpy()\n",
    "\n",
    "    test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n",
    "    \n",
    "    del model\n",
    "    return test_pred\n",
    "\n",
    "\n",
    "def folds_inference(path, test_dataset):\n",
    "    batch_size = BERT_LARGE_BATCH_SIZE if 'large' in path else BERT_BASE_BATCH_SIZE\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    bert_config = BertConfig(path + 'bert_config.json')\n",
    "    models = glob.glob(path + '*bert*.bin*')\n",
    "    test_pred = []\n",
    "    \n",
    "    for model_path in models:\n",
    "        if model_path in MODELS_TO_SKIP:\n",
    "            continue\n",
    "        print('Inference:',model_path)\n",
    "        \n",
    "        test_pred.append(bert_model_inference(bert_config, model_path, test_loader, batch_size))\n",
    "    test_pred = np.average(test_pred, axis=0)\n",
    "    \n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:5201: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/.keras/keras.json' mode='r' encoding='UTF-8'>\n",
      "  _config = json.load(open(_config_path))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "CRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'\n",
    "GLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\n",
    "\n",
    "\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220\n",
    "\n",
    "lstm_models = []\n",
    "TRAIN_MAX_FEAURES = 410047"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 5.36 s, total: 1min 7s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path,'rb') as f:\n",
    "        emb_arr = pickle.load(f)\n",
    "    return emb_arr\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((max_features + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    embedding_matrix[i] = embedding_index[word.lower()]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        embedding_matrix[i] = embedding_index[word.title()]\n",
    "                    except KeyError:\n",
    "                        unknown_words.append(word)\n",
    "                        try:\n",
    "                            embedding_matrix[i] = embedding_index[word.capitalize()]\n",
    "                        except KeyError:\n",
    "                            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\n",
    "symbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'\n",
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "small_caps_mapping = { \n",
    "\"ᴀ\": \"a\", \"ʙ\": \"b\", \"ᴄ\": \"c\", \"ᴅ\": \"d\", \"ᴇ\": \"e\", \"ғ\": \"f\", \"ɢ\": \"g\", \"ʜ\": \"h\", \"ɪ\": \"i\", \n",
    "\"ᴊ\": \"j\", \"ᴋ\": \"k\", \"ʟ\": \"l\", \"ᴍ\": \"m\", \"ɴ\": \"n\", \"ᴏ\": \"o\", \"ᴘ\": \"p\", \"ǫ\": \"q\", \"ʀ\": \"r\", \n",
    "\"s\": \"s\", \"ᴛ\": \"t\", \"ᴜ\": \"u\", \"ᴠ\": \"v\", \"ᴡ\": \"w\", \"x\": \"x\", \"ʏ\": \"y\", \"ᴢ\": \"z\"}\n",
    "contraction_mapping = {\n",
    "\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "\"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "\"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
    "\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \n",
    "\"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "\"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n",
    "\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n",
    "\"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "\"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\":\"this is\",\"that'd\": \"that would\", \n",
    "\"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
    "\"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n",
    "\"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
    "\"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n",
    "\"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "\"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "\"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
    "\"trump's\": \"trump is\", \"obama's\": \"obama is\", \"canada's\": \"canada is\", \"today's\": \"today is\"}\n",
    "specail_signs = { \"…\": \"...\", \"₂\": \"2\"}\n",
    "specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def preprocess(x):\n",
    "    x = handle_punctuation(x)\n",
    "    x = handle_contractions(x)\n",
    "    x = fix_quote(x)\n",
    "    x = ' '.join(re.findall(r'[A-Z]?[^A-Z\\s]+|[A-Z]+', x))\n",
    "    x = re.sub(r'[A-Za-z]+://[A-Za-z0-9-_]+.[A-Za-z0-9-_:%&;\\?#/.=]+', ' https ', x)\n",
    "    x = correct_misspel(isolate(remove_diacritics(x)))\n",
    "    return x\n",
    "\n",
    "x_test = test_df['comment_text'].apply(lambda x:preprocess(x))\n",
    "\n",
    "max_features = 410047\n",
    "tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)\n",
    "tokenizer.fit_on_texts(list(x_test))\n",
    "\n",
    "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
    "\n",
    "max_features = max_features or len(tokenizer.word_index) + 1\n",
    "\n",
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "del crawl_matrix\n",
    "del glove_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x, lengths=None):\n",
    "        h_embedding = self.embedding(x.long())\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        return out\n",
    "\n",
    "def lstm_predict(path, batch_size, test_size):\n",
    "    model = NeuralNet(embedding_matrix, 6)\n",
    "    model.to(device)\n",
    "    tmp_state = torch.load(path, map_location=device)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'embedding' not in name:\n",
    "            param.data = tmp_state[name]  \n",
    "            \n",
    "    del tmp_state\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()        \n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    test_preds = np.zeros(test_size)\n",
    "    \n",
    "    for i, x_batch in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            X = x_batch[0].cuda()\n",
    "            y_pred = sigmoid(model(X).detach().cpu().numpy()[:,0])\n",
    "            test_preds[i * batch_size:(i+1) * batch_size] = y_pred\n",
    "               \n",
    "    return test_preds\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.87 s, sys: 28 ms, total: 4.9 s\n",
      "Wall time: 4.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 1024\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=220))\n",
    "test_loader = data.DataLoader(data.TensorDataset(x_test_padded), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/lstm-bin/lstm_fold3_epoch4.bin\n",
      "Inference: ../input/lstm-bin/lstm_fold0_epoch4.bin\n",
      "Inference: ../input/lstm-bin/lstm_fold1_epoch4.bin\n",
      "Inference: ../input/lstm-bin/lstm_fold2_epoch4.bin\n",
      "CPU times: user 27.2 s, sys: 30.4 s, total: 57.6 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lstm_bin_preds = []\n",
    "lstm_bin_models = glob.glob('../input/lstm-bin/*.bin')\n",
    "\n",
    "for model_path in lstm_bin_models:\n",
    "    print('Inference:', model_path)\n",
    "    lstm_bin_preds.append(lstm_predict(model_path, batch_size, len(x_test)))\n",
    "    \n",
    "lstm_bin_preds = np.average(lstm_bin_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.14304568e-02, 6.59104826e-05, 6.56928268e-03, 2.41121446e-03,\n",
       "       9.88708407e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_bin_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/lstm-cont/lstm_fold3_epoch4.bin\n",
      "Inference: ../input/lstm-cont/lstm_fold0_epoch4.bin\n",
      "Inference: ../input/lstm-cont/lstm_fold1_epoch4.bin\n",
      "Inference: ../input/lstm-cont/lstm_fold2_epoch4.bin\n",
      "CPU times: user 24 s, sys: 29.6 s, total: 53.6 s\n",
      "Wall time: 53.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lstm_cont_preds = []\n",
    "lstm_cont_models = glob.glob('../input/lstm-cont/*.bin')\n",
    "\n",
    "for model_path in lstm_cont_models:\n",
    "    print('Inference:', model_path)\n",
    "    lstm_cont_preds.append(lstm_predict(model_path, batch_size, len(x_test)))\n",
    "    \n",
    "lstm_cont_preds = np.average(lstm_cont_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03682656, 0.00694891, 0.04743195, 0.03628699, 0.76618168])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_cont_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del embedding_matrix, x_test, x_test_padded, test_loader\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binarized BERTs inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(LARGE_BIN_PATH, cache_dir=None,do_lower_case=True)\n",
    "X_test, lens = convert_lines_bin(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "\n",
    "sorted_idx = np.argsort(lens)\n",
    "X_test = X_test[sorted_idx]\n",
    "\n",
    "test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/large-bin-all/1_bert_pytorch_15_binary_all_3.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1521/1521 [11:38<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/large-bin-all/1_bert_pytorch_15_binary_all_2.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1521/1521 [11:38<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/base-bin-all/1_bert_pytorch_15_binary_all_base_2.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [03:35<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/base-bin-all/1_bert_pytorch_15_binary_all_base_3.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [03:36<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/base-bin-all/1_bert_pytorch_15_binary_all_base_1.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [03:36<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/base-bin-all/1_bert_pytorch_15_binary_all_base_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [03:36<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pred = folds_inference(LARGE_BIN_PATH, test)\n",
    "large_bin_preds = np.zeros((len(X_test)))\n",
    "large_bin_preds[sorted_idx] = test_pred\n",
    "\n",
    "test_pred = folds_inference(BASE_BIN_PATH, test)\n",
    "base_bin_preds = np.zeros((len(X_test)))\n",
    "base_bin_preds[sorted_idx] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.65564321e-04, 6.13849207e-04, 4.49121751e-03, 4.17019566e-04,\n",
       "       9.91272908e-01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_bin_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.50057844e-04, 3.42046558e-04, 5.34829549e-03, 1.02787744e-03,\n",
       "       9.89213815e-01])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_bin_preds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continous BERTs inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained(LARGE_CONT_PATH, cache_dir=None,do_lower_case=True)\n",
    "X_test, lens = convert_lines_cont(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "\n",
    "sorted_idx = np.argsort(lens)\n",
    "X_test = X_test[sorted_idx]\n",
    "\n",
    "test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/large-cont-all/1_bert_pytorch_14_NewLoss_all_3.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1521/1521 [11:30<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/large-cont-all/1_bert_pytorch_14_NewLoss_all_2.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1521/1521 [11:30<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/base-cont-all/1_bert_pytorch_14_NewLoss_all_base_2.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [03:34<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/base-cont-all/1_bert_pytorch_14_NewLoss_all_base_1.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [03:33<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/base-cont-all/1_bert_pytorch_14_NewLoss_all_base_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [03:33<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference: ../input/base-cont-all/1_bert_pytorch_14_NewLoss_all_base_3.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 761/761 [03:33<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pred = folds_inference(LARGE_CONT_PATH, test)\n",
    "large_cont_preds = np.zeros((len(X_test)))\n",
    "large_cont_preds[sorted_idx] = test_pred\n",
    "\n",
    "test_pred = folds_inference(BASE_CONT_PATH, test)\n",
    "base_cont_preds = np.zeros((len(X_test)))\n",
    "base_cont_preds[sorted_idx] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e6c9a88569a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "del test_pred, test, X_test, lens, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00915421, 0.00407942, 0.0651625 , 0.01436458, 0.74449154])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_cont_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01757531, 0.00528343, 0.06148341, 0.02968971, 0.71651137])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_cont_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.475741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.140457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.647026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.473249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.983892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7000000    0.475741\n",
       "1  7000001    0.140457\n",
       "2  7000002    0.647026\n",
       "3  7000003    0.473249\n",
       "4  7000004    0.983892"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final_pred = (large_bin_preds + base_bin_preds + large_cont_preds + base_cont_preds + lstm_bin_preds + lstm_cont_preds)/6\n",
    "cont_preds = 4 * large_cont_preds + 4 * base_cont_preds + lstm_cont_preds\n",
    "bin_preds = 4 * large_bin_preds + 4 * base_bin_preds + lstm_bin_preds\n",
    "final_pred = rankdata(cont_preds) + rankdata(bin_preds)\n",
    "final_pred = final_pred/np.max(final_pred)\n",
    "\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_df['id'],\n",
    "    'prediction': final_pred\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
